version: "3.8"
services:

  ollama:
    image: ollama/ollama:0.1.24
    expose:
     - 11434/tcp
    healthcheck:
      test: ollama --version || exit 1
    command: serve
    restart: on-failure
    volumes:
      - ollama:/root/.ollama
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['all']
              capabilities: [gpu]

  llava:
    build:
      context: ./llava
    env_file:
      - .env
    restart: on-failure
    depends_on:
     ollama:
       condition: service_healthy
     curl:
       condition: service_completed_successfully

  curl:
    image: curlimages/curl:latest
    env_file:
      - .env
    command: >
      /bin/sh -c "curl -X POST $$OLLAMA_BASE_URL/api/pull
      -d \"{\\\"name\\\": \\\"$$VISION_MODEL\\\"}\""
    restart: on-failure
    depends_on:
      - ollama

volumes:
  ollama:
